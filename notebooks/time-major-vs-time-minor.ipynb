{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from tempfile import NamedTemporaryFile\n",
    "from gc import collect\n",
    "from time import sleep\n",
    "sys.path.append('../util')\n",
    "from meters import ThroughputMeter, clear_host_cache\n",
    "from ncgen import make_nc\n",
    "from grids import *\n",
    "import netCDF4\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_netcdf_file(timescale, time_major=True, grid=canada_5k):\n",
    "    print(\"Creating a time-{} NetCDF file with {}x{} grid and {} time steps\".format('major' if time_major else 'minor', grid['lon']['count'], grid['lat']['count'],len(timescale)))\n",
    "    with NamedTemporaryFile(suffix='.nc', delete=False, dir='/app/tmp') as f:\n",
    "        nc = make_nc(f.name, grid=grid, timescale=timescale, timemajor=time_major)\n",
    "        nc.close()\n",
    "    print(\"File size: {:.2f}Mb\".format(os.path.getsize(f.name)/1024/1024))\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "../tmp path in the Docker container points at rotating media storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def netcdf_read_test(f, time_major):\n",
    "    results = []\n",
    "    # Open the file just created\n",
    "    nc = netCDF4.Dataset(f.name, 'r')\n",
    "    if time_major:\n",
    "        with ThroughputMeter() as t:\n",
    "            a = nc.variables['var_0'][0,:,:]\n",
    "\n",
    "    else:\n",
    "        with ThroughputMeter() as t:\n",
    "            a = nc.variables['var_0'][:,:,0]\n",
    "    \n",
    "    results.append((time_major, len(timescale), t.megabytes_per_second(a)))\n",
    "\n",
    "    # python-netCDF4 seems to leak file descriptors\n",
    "    # We have to take a lot of steps to make sure that the files get closed and that\n",
    "    # the space gets reclaimed by the OS\n",
    "    nc.close\n",
    "    del nc\n",
    "    print(\"Removing {}\".format(f.name))\n",
    "    os.remove(f.name)\n",
    "    f.close()\n",
    "    collect()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for time_major in [True, False]:\n",
    "    for grid in [world_250k, world_125k, canada_5k, bc_400m]:\n",
    "        for timescale in [timescales['annual']]:\n",
    "            testfile = write_netcdf_file(timescale, time_major=time_major, grid=grid)\n",
    "            clear_host_cache()\n",
    "            results.append(netcdf_read_test(testfile, time_major))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for time_major in [True, False]:\n",
    "    for grid in [world_250k, world_125k, canada_5k, bc_400m]:\n",
    "        for timescale in [timescales['monthly']]:\n",
    "            testfile = write_netcdf_file(timescale, time_major=time_major, grid=grid)\n",
    "            clear_host_cache()\n",
    "            results.append(netcdf_read_test(testfile, time_major))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daily - omit BC400m grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Daily timescale. Omit bc400m grid\n",
    "for time_major in [True, False]:\n",
    "    for grid in [world_250k, world_125k, canada_5k]:\n",
    "        for timescale in [timescales['daily']]:\n",
    "            testfile = write_netcdf_file(timescale, time_major=time_major, grid=grid)\n",
    "            clear_host_cache()\n",
    "            results.append(netcdf_read_test(testfile, time_major))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "monthly_grid_sizes = ['world_125k', 'canada_5k', 'bc_400m']\n",
    "daily_grid_sizes = ['world_250k', 'canada_5k', 'bc_400m']\n",
    "\n",
    "monthly_step_read_throughput_time_major = [results_tmaj_world_125k_monthly, results_tmaj_canada_5k_monthly, \\\n",
    "                            results_tmaj_bc_400m_monthly]\n",
    "monthly_step_read_throughput_time_minor = [results_tmin_world_125k_monthly, results_tmin_canada_5k_monthly, \\\n",
    "                            results_tmin_bc_400m_monthly]\n",
    "\n",
    "daily_step_read_throughput_time_major = [results_tmaj_world_250k_daily, results_tmaj_canada_5k_daily, \\\n",
    "                            results_tmaj_bc_400m_daily]\n",
    "\n",
    "daily_step_read_throughput_time_minor = [results_tmin_world_250k_daily, results_tmin_canada_5k_daily, \\\n",
    "                            results_tmin_bc_400m_daily]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "monthly_tmaj, = plt.plot([point[0][2] for point in monthly_step_read_throughput_time_major], label='time-major')\n",
    "monthly_tmin, = plt.plot([point[0][2] for point in monthly_step_read_throughput_time_minor], label='time-minor')\n",
    "plt.xticks(range(len(monthly_grid_sizes)), monthly_grid_sizes)\n",
    "plt.title('Monthly (1800 time steps) Throughput Comparison [MB/s]')\n",
    "plt.legend(loc='upper center', shadow=True)\n",
    "plt.show()\n",
    "\n",
    "daily_tmaj, = plt.plot([point[0][2] for point in daily_step_read_throughput_time_major], label='time-major')\n",
    "daily_tmin, = plt.plot([point[0][2] for point in daily_step_read_throughput_time_minor], label='time-minor')\n",
    "plt.xticks(range(len(daily_grid_sizes)), daily_grid_sizes)\n",
    "plt.title('Daily (54787 time steps) Throughput Comparison [MB/s]')\n",
    "plt.legend(loc='upper center', shadow=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There seems to be a slightly higher throughput for reading out of large time-major NetCDF data files vs. time-minor (on the order of ~7MB/sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
